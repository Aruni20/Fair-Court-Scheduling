version: '3.8'

services:

  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init-airflow.sql:/docker-entrypoint-initdb.d/init-airflow.sql
    ports:
      - "3306:3306"
    networks:
      - custom_network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5

  mongodb:
    image: mongo:6
    container_name: mongodb
    environment:
      MONGO_INITDB_DATABASE: court_db
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - custom_network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - custom_network

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    networks:
      - custom_network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    ports:
      - "8085:8080"
    networks:
      - custom_network

  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=test
    volumes:
      - hdfs_namenode_data:/hadoop/dfs/name
    ports:
      - "9870:9870"
    networks:
      - custom_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      retries: 5

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    depends_on:
      - hdfs-namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
    volumes:
      - hdfs_datanode_data:/hadoop/dfs/data
    ports:
      - "9864:9864"
    networks:
      - custom_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864"]
      interval: 30s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: minio
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9002:9001"
    volumes:
      - minio_data:/data
    networks:
      - custom_network

  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9001:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    networks:
      - custom_network

  spark:
    image: bitnami/spark:3.4.0
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8080"
      - "7077:7077"
    networks:
      - custom_network

  spark-worker:
    image: bitnami/spark:3.4.0
    container_name: spark-worker
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    networks:
      - custom_network

  airflow-init:
    image: apache/airflow:2.7.1
    container_name: airflow-init
    depends_on:
      - mysql
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql://airflow:airflow@mysql:3306/airflow
    command: bash -c "airflow db init"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/data             # ✅ mimic reports/scheduled
    networks:
      - custom_network

  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/data             # ✅ so DAGs can write reports
      - ./app:/app               # ✅ optional, if calling app logic
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - custom_network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      retries: 5

  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql://airflow:airflow@mysql:3306/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/data             # ✅ shared mimic data
      - ./app:/app
    command: scheduler
    networks:
      - custom_network

  streamlit:
    image: court_pipeline-streamlit
    container_name: streamlit
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./app:/app
      - ./data:/data             # ✅ reads fake flags/reports
    command: streamlit run /app/dashboard.py
    networks:
      - custom_network

networks:
  custom_network:
    name: court_pipeline_custom_network

volumes:
  mysql_data:
  mongodb_data:
  hdfs_namenode_data:
  hdfs_datanode_data:
  minio_data:
  clickhouse_data:
